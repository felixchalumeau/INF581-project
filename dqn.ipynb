{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1239..1553 -> 314-tiles track\n"
     ]
    }
   ],
   "source": [
    "# test the environment\n",
    "import gym\n",
    "env = gym.make('CarRacing-v0')\n",
    "\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, sgd\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import pickle, os, gzip\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape : (96, 96, 3)\n",
      "New shape : (1, 96, 96, 3)\n",
      "New prrocessed shape : (1, 84, 84, 1)\n",
      "Continuous action shape : 3\n",
      "Action shape : 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "processed_shape = (84,84,1)\n",
    "new_shape = (1,)+state_size\n",
    "new_processed_shape= (1,)+processed_shape\n",
    "continuous_action_size = env.action_space.shape[0]\n",
    "action_size = 4\n",
    "\n",
    "print(\"State shape : \" + str(state_size))\n",
    "print(\"New shape : \" + str(new_shape))\n",
    "print(\"New prrocessed shape : \" + str(new_processed_shape))\n",
    "print(\"Continuous action shape : \" + str(continuous_action_size))\n",
    "print(\"Action shape : \" + str(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_from_discrete(action):\n",
    "    if (action==0):\n",
    "        return [-1, 0, 0]\n",
    "    if (action==1):\n",
    "        return [1, 0, 0]\n",
    "    if (action==2):\n",
    "        return [0, 0, 0]\n",
    "    #other actions to take into account [-1, 0, 0.5]  [1, 0, 0.5]  [0, 1, 0]  [0, 0, 0.5]\n",
    "    return [0, 1, 0]\n",
    "\n",
    "def discrete_from_continuous(action):\n",
    "    if (action[0] == -1):\n",
    "        return 0\n",
    "    if (action[0] == 1):\n",
    "        return 1\n",
    "    if (action[1] == 0):\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "def onehot_from_discrete(action):\n",
    "    return [ 1 if i==action else 0 for i in range(action_size)]\n",
    "\n",
    "def discrete_from_onehot(onehot):\n",
    "    for i in range(len(onehot)):\n",
    "        if (onehot[i]==1):\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_from_onehot(onehot_from_discrete(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    return np.reshape(state[:,:84,6:90,1], new_processed_shape)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters = 32, kernel_size=8, strides=4, activation='relu', input_shape=processed_shape))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Conv2D(filters = 64, kernel_size=4, strides=2, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',metrics=['accuracy'], optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        #if np.random.rand() <= self.epsilon:\n",
    "        #    return random.randrange(self.action_size)\n",
    "        state = preprocess(state)\n",
    "        act_values = self.model.predict(state)[0]\n",
    "        return np.argmax(act_values)  # returns action\n",
    "    \n",
    "    def in_grass(self, state):\n",
    "        return state[0,82,42,1]>150\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            next_state = preprocess(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def train(self, memory):\n",
    "        batch_size=100\n",
    "        states = []\n",
    "        target_fs = []\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            next_state = preprocess(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state)\n",
    "            target_fs.append(target_f)\n",
    "        history = self.model.fit(np.array(states)[:,0,:,:,:],\n",
    "                                 np.array(target_fs)[:,0,:],\n",
    "                                 batch_size=batch_size,\n",
    "                                 verbose=0)\n",
    "        return history\n",
    "\n",
    "    \"\"\"\n",
    "    # model and train for one hot encoding\n",
    "\n",
    "    def _build_model(self):\n",
    "        # https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters = 32, kernel_size=(8,8), strides=(4,4), activation='relu', input_shape=processed_shape))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Conv2D(filters = 64, kernel_size=(3,3), strides=(2,2), activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, memory):\n",
    "        states = []\n",
    "        actions = []\n",
    "        for state, action, reward, next_state, done in memory:\n",
    "            state = preprocess(state)\n",
    "            action = onehot_from_discrete(action)\n",
    "            states.append(state)\n",
    "            actions.append([action])\n",
    "        states = np.array(states)[:,0,:,:,:]\n",
    "        actions = np.array(actions)[:,0,:]\n",
    "        history = self.model.fit(states, actions, batch_size=100, epochs=100, validation_split=0.2)\n",
    "        return history \"\"\"\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANVElEQVR4nO3df6jd9X3H8ed7yU3vNJZEpy4mskQI7YLQ2V2drWOoaVnrSpVih6WUbBPuP9tqf0CN2x9l/1Uo1SKjcNGVUKS1s2GKlBZJU2EgmTcqqzG1ZlpjaqoWYquFsPx474/zvdn19l7vyf2ec8859/18wOWc749zz9uved3398fne05kJpJWvt8bdAGSlodhl4ow7FIRhl0qwrBLRRh2qYhWYY+Ij0TEcxFxKCJ29qooSb0XS73OHhGrgJ8BHwaOAE8An8rMZ3tXnqReWd3itVcBhzLzBYCI+A5wI7Bg2MfXjed5l5zX4i01r9cHXUBLFw66gJXjzVfe5Pgbx2O+ZW3CvhF4edb0EeDP5q4UEZPAJMDaP1zLJ771iRZvqXlNDbqAliYHXcDKsfszuxdc1uaYfb6/Hr9zTJCZU5k5kZkT4+vHW7ydpDbahP0IcOms6U3AK+3KkdQvbcL+BLA1IrZExBrgFuDh3pQlqdeWfMyemScj4h+AHwKrgH/LzAM9q0xST7U5QUdmfh/4fo9qkdRHrcKu4TA2NjboElo5wYlBl1CCw2WlIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSrCsEtFGHapCO9nXwFOnz496BI0AuzsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIb4RZAU6dOjXoEjQC7OxSEYZdKsKwS0UYdqkIwy4VYdilIrz0JvXKhh79nqM9+j1zLNrZI+LSiNgbEQcj4kBE3NbMPz8iHo2I55vH9f0pUVIvdNPZTwJfzMwnI+I8YH9EPAr8DbAnM78SETuBncDt/StV6qFedeERsmhnz8yjmflk8/xN4CCwEbgR2NWstgu4qV9FSmrvrE7QRcRm4ApgH3BxZh6Fzh8E4KIFXjMZEdMRMX382PF21Upasq5P0EXEWuB7wOcy8zcR0dXrMnMKmAK4cNuFuZQipYq73b3WVWePiDE6Qb8/M3c3s1+NiA3N8g3Aa/0pUVIvdHM2PoD7gIOZ+bVZix4GdjTPdwAP9b48qXGUvl2SGjob6MueTDe78dcAnwF+EhFPN/P+CfgK8N2IuBU4DHyy9+VJ6pVFw56Z/wksdIC+vbflaCnGxsYGXUIrJzgx6BJKcLisVITDZUeFZ6PVkp1dKsKwS0W4G98v7nZryNjZpSLs7DPsxFrh7OxSESu3s9uppbexs0tFGHapCMMuFWHYpSIMu1SEYZeKWLmX3go5ffr0oEvQCLCzS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhHe9abTM/o72lf6horP/+3rw3fR2dqmIldvZC3WAU6dODboEjQA7u1SEYZeKMOxSEYZdKqLrsEfEqoh4KiIeaaa3RMS+iHg+Ih6IiDX9K1NSW2fT2W8DDs6avhO4KzO3AseAW3tZmKTe6irsEbEJ+Cvg3mY6gOuBB5tVdgE39aNASb3RbWe/G/gSMPMB5RcAb2TmyWb6CLBxvhdGxGRETEfE9PFjx1sVK2npFg17RHwMeC0z98+ePc+qOd/rM3MqMycyc2J8/fgSy5TUVjcj6K4BPh4RNwDjwLvpdPp1EbG66e6bgFf6V6akthbt7Jl5R2ZuyszNwC3AjzLz08Be4OZmtR3AQ32rUlJrba6z3w58ISIO0TmGv683JUnqh7O6ESYzfwz8uHn+AnBV70uS1A+OoJOKMOxSESv3fvZCxsbGBl1CKyc4MegSSrCzS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhHe9rQCnT59efCWNtplvIm7xPe12dqmIGp195q/hCv+e9nJG+f9riw69VHZ2qYganV1qawCduNfs7FIRdnatPCugC/eDnV0qwrBLRbgbr9HnbntX7OxSEYZdKsKwS0UYdqkIwy4V4dn4YdDybPKpU6d6U4dWNDu7VIRhl4pwN34pHMShEWRnl4roqrNHxDrgXuByIIG/A54DHgA2Az8H/jozj/Wlyl6xI6uwbjv714EfZOZ7gfcBB4GdwJ7M3ArsaaYlDalFwx4R7wb+ArgPIDP/NzPfAG4EdjWr7QJu6leRktrrprNfBrwOfDMinoqIeyPiXODizDwK0DxeNN+LI2IyIqYjYvr4seM9K1zS2ekm7KuB9wPfyMwrgN9yFrvsmTmVmROZOTG+fnyJZUpqq5uwHwGOZOa+ZvpBOuF/NSI2ADSPr/WnREm9sGjYM/OXwMsR8Z5m1nbgWeBhYEczbwfwUF8qlNQT3Q6q+Ufg/ohYA7wA/C2dPxTfjYhbgcPAJ/tTolasyUEXUEtXYc/Mp4GJeRZt7205kvrFEXRSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRfgadzpicmupqvalJx7mOIju7VISdXWfMvvlh+h2W2ddHk51dKsKwS0UYdqkIwy4V4Qk6nTE5+5LanMtwk15uG3l2dqkIwy4VYdilIjxm1xnvNFx29jKHy44mO7tUhGGXijDsUhGGXSrCE3Q6w7veVjY7u1SEnV1nzO3mCy3r7vNsNGzs7FIRhl0qwrBLRRh2qQhP0OkMx7yvbHZ2qQjDLhVh2KUiugp7RHw+Ig5ExDMR8e2IGI+ILRGxLyKej4gHImJNv4uVtHSLhj0iNgKfBSYy83JgFXALcCdwV2ZuBY4Bt/azUEntdLsbvxr4/YhYDZwDHAWuBx5slu8Cbup9eZJ6ZdGwZ+YvgK8Ch+mE/NfAfuCNzDzZrHYE2NivIiW1181u/HrgRmALcAlwLvDReVbNBV4/GRHTETF9/NjxNrVKaqGb3fgPAS9m5uuZeQLYDXwQWNfs1gNsAl6Z78WZOZWZE5k5Mb5+vCdFSzp73YT9MHB1RJwTEQFsB54F9gI3N+vsAB7qT4mSeqGbY/Z9dE7EPQn8pHnNFHA78IWIOARcANzXxzoltdTV2PjM/DLw5TmzXwCu6nlFkvrCG2GkUbBhgflHu/8VDpeVijDsUhGROe/l8f68WcTyvZlUVGbGfPPt7FIRhl0qwrBLRRh2vbNkgbseNGoMu1SEYZeKMOxSEYZdKmLgY+O3bdsGwIEDBwDo3EULswf7bN68GYCXXnrpbctm1p27/txlkuzsUhkDHy771ltvAbB27doFX7dQ1169+v93TE6ePPm23zPze9XSzKZ3R2lkOFxWKm7gx+x79uxZdJ29e/cCcN11171t/pVXXnnm+eOPPw7Y0aWF2NmlIgy7VMTAT9DNdxltoXUee+wxAK699trfWWfmEt727dsBuOeee9oVK40oT9BJxQ28s0vqLTu7VNzAL731y3x7LL0aQvunXdzgvd9RKBoydnapCI/ZpRXGY3apOMMuFWHYpSIMu1TEcl96+xXw2+ZxlPwBo1czjGbd1tzOHy20YFnPxgNExHRmTizrm7Y0ijXDaNZtzf3jbrxUhGGXihhE2KcG8J5tjWLNMJp1W3OfLPsxu6TBcDdeKsKwS0UsW9gj4iMR8VxEHIqIncv1vmcrIi6NiL0RcTAiDkTEbc388yPi0Yh4vnlcP+ha54qIVRHxVEQ80kxviYh9Tc0PRMSaQdc4W0Ssi4gHI+Knzfb+wIhs5883/zaeiYhvR8T4sG9rWKawR8Qq4F+BjwLbgE9FxLbleO8lOAl8MTP/GLga+Pum1p3AnszcCuxppofNbcDBWdN3Anc1NR8Dbh1IVQv7OvCDzHwv8D46tQ/1do6IjcBngYnMvBxYBdzC8G/rzoc89PsH+ADww1nTdwB3LMd796D2h4APA88BG5p5G4DnBl3bnDo30QnH9cAjdL7D5VfA6vn+Hwz6B3g38CLNSeJZ84d9O28EXgbOpzMC9RHgL4d5W8/8LNdu/MwGmnGkmTfUImIzcAWwD7g4M48CNI8XDa6yed0NfAk43UxfALyRmSeb6WHb5pcBrwPfbA497o2Icxny7ZyZvwC+ChwGjgK/BvYz3NsaWL5j9vluph/qa34RsRb4HvC5zPzNoOt5JxHxMeC1zNw/e/Y8qw7TNl8NvB/4RmZeQeeeiaHaZZ9Pcw7hRmALcAlwLp3D07mGaVsDyxf2I8Cls6Y3Aa8s03uftYgYoxP0+zNzdzP71YjY0CzfALw2qPrmcQ3w8Yj4OfAdOrvydwPrImLmZqdh2+ZHgCOZua+ZfpBO+Id5OwN8CHgxM1/PzBPAbuCDDPe2BpYv7E8AW5szlmvonNB4eJne+6xE51Mp7wMOZubXZi16GNjRPN9B51h+KGTmHZm5KTM309m2P8rMTwN7gZub1Yat5l8CL0fEe5pZ24FnGeLt3DgMXB0R5zT/VmbqHtptfcYynti4AfgZ8D/APw/6ZMU71PnndHbB/ht4uvm5gc4x8B7g+ebx/EHXukD91wKPNM8vA/4LOAT8O/CuQdc3p9Y/Aaabbf0fwPpR2M7AvwA/BZ4BvgW8a9i3dWY6XFaqwhF0UhGGXSrCsEtFGHapCMMuFWHYpSIMu1TE/wGz8CZZeZ9KAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(observation)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(observation[:84,6:90,1] , cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "warm_up = 50\n",
    "batch_size = 100\n",
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    #get the data needed from the gzip\n",
    "    print(\"Reading data...\")\n",
    "    with gzip.open('./data/data.pkl.gzip','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    states = data[\"state\"]\n",
    "    next_states = data[\"next_state\"]\n",
    "    actions = data[\"action\"]\n",
    "    rewards = data[\"reward\"]\n",
    "    dones = data[\"terminal\"]\n",
    "    \n",
    "    # put it in the good format, as in the agent memory\n",
    "    memory = deque(maxlen=100000)\n",
    "    NB_TRIALS = len(states)\n",
    "    for ii in range(NB_TRIALS):\n",
    "        for jj in range(warm_up,len(states[ii])):\n",
    "            state = states[ii][jj]\n",
    "            state = np.reshape(state, new_shape)\n",
    "            action = discrete_from_continuous(actions[ii][jj])\n",
    "            reward = rewards[ii][jj]\n",
    "            next_state = next_states[ii][jj]\n",
    "            next_state = np.reshape(next_state, new_shape)\n",
    "            done = dones[ii][jj]\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pierr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               663680    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 699,108\n",
      "Trainable params: 699,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Training...\n",
      "----------- 10/1000 ------ val_loss : [0.32]\n",
      "----------- 20/1000 ------ val_loss : [0.39]\n",
      "----------- 30/1000 ------ val_loss : [0.38]\n",
      "----------- 40/1000 ------ val_loss : [0.35]\n",
      "----------- 50/1000 ------ val_loss : [0.45]\n",
      "----------- 60/1000 ------ val_loss : [0.82]\n",
      "----------- 70/1000 ------ val_loss : [0.86]\n",
      "----------- 80/1000 ------ val_loss : [0.37]\n",
      "----------- 90/1000 ------ val_loss : [0.61]\n",
      "----------- 100/1000 ------ val_loss : [0.6]\n",
      "----------- 110/1000 ------ val_loss : [0.44]\n",
      "----------- 120/1000 ------ val_loss : [0.36]\n",
      "----------- 130/1000 ------ val_loss : [0.39]\n",
      "----------- 140/1000 ------ val_loss : [0.35]\n",
      "----------- 150/1000 ------ val_loss : [0.44]\n",
      "----------- 160/1000 ------ val_loss : [0.43]\n",
      "----------- 170/1000 ------ val_loss : [0.7]\n",
      "----------- 180/1000 ------ val_loss : [0.78]\n",
      "----------- 190/1000 ------ val_loss : [0.69]\n",
      "----------- 200/1000 ------ val_loss : [0.86]\n",
      "----------- 210/1000 ------ val_loss : [0.84]\n",
      "----------- 220/1000 ------ val_loss : [0.83]\n",
      "----------- 230/1000 ------ val_loss : [0.8]\n",
      "----------- 240/1000 ------ val_loss : [0.81]\n",
      "----------- 250/1000 ------ val_loss : [0.88]\n",
      "----------- 260/1000 ------ val_loss : [0.88]\n",
      "----------- 270/1000 ------ val_loss : [0.91]\n",
      "----------- 280/1000 ------ val_loss : [0.88]\n",
      "----------- 290/1000 ------ val_loss : [0.82]\n",
      "----------- 300/1000 ------ val_loss : [0.89]\n",
      "----------- 310/1000 ------ val_loss : [0.74]\n",
      "----------- 320/1000 ------ val_loss : [0.9]\n",
      "----------- 330/1000 ------ val_loss : [0.85]\n",
      "----------- 340/1000 ------ val_loss : [0.87]\n",
      "----------- 350/1000 ------ val_loss : [0.87]\n",
      "----------- 360/1000 ------ val_loss : [0.92]\n",
      "----------- 370/1000 ------ val_loss : [0.8]\n",
      "----------- 380/1000 ------ val_loss : [0.81]\n",
      "----------- 390/1000 ------ val_loss : [0.79]\n",
      "----------- 400/1000 ------ val_loss : [0.81]\n",
      "----------- 410/1000 ------ val_loss : [0.93]\n",
      "----------- 420/1000 ------ val_loss : [0.83]\n",
      "----------- 430/1000 ------ val_loss : [0.82]\n",
      "----------- 440/1000 ------ val_loss : [0.83]\n",
      "----------- 450/1000 ------ val_loss : [0.84]\n",
      "----------- 460/1000 ------ val_loss : [0.81]\n",
      "----------- 470/1000 ------ val_loss : [0.84]\n",
      "----------- 480/1000 ------ val_loss : [0.78]\n",
      "----------- 490/1000 ------ val_loss : [0.81]\n",
      "----------- 500/1000 ------ val_loss : [0.85]\n",
      "----------- 510/1000 ------ val_loss : [0.86]\n",
      "----------- 520/1000 ------ val_loss : [0.81]\n",
      "----------- 530/1000 ------ val_loss : [0.83]\n",
      "----------- 540/1000 ------ val_loss : [0.87]\n",
      "----------- 550/1000 ------ val_loss : [0.87]\n",
      "----------- 560/1000 ------ val_loss : [0.84]\n",
      "----------- 570/1000 ------ val_loss : [0.83]\n",
      "----------- 580/1000 ------ val_loss : [0.83]\n",
      "----------- 590/1000 ------ val_loss : [0.83]\n",
      "----------- 600/1000 ------ val_loss : [0.89]\n",
      "----------- 610/1000 ------ val_loss : [0.85]\n",
      "----------- 620/1000 ------ val_loss : [0.85]\n",
      "----------- 630/1000 ------ val_loss : [0.86]\n",
      "----------- 640/1000 ------ val_loss : [0.87]\n",
      "----------- 650/1000 ------ val_loss : [0.91]\n",
      "----------- 660/1000 ------ val_loss : [0.92]\n",
      "----------- 670/1000 ------ val_loss : [0.89]\n",
      "----------- 680/1000 ------ val_loss : [0.89]\n",
      "----------- 690/1000 ------ val_loss : [0.88]\n",
      "----------- 700/1000 ------ val_loss : [0.91]\n",
      "----------- 710/1000 ------ val_loss : [0.89]\n",
      "----------- 720/1000 ------ val_loss : [0.84]\n",
      "----------- 730/1000 ------ val_loss : [0.87]\n",
      "----------- 740/1000 ------ val_loss : [0.82]\n",
      "----------- 750/1000 ------ val_loss : [0.9]\n",
      "----------- 760/1000 ------ val_loss : [0.92]\n",
      "----------- 770/1000 ------ val_loss : [0.84]\n",
      "----------- 780/1000 ------ val_loss : [0.87]\n",
      "----------- 790/1000 ------ val_loss : [0.89]\n",
      "----------- 800/1000 ------ val_loss : [0.91]\n",
      "----------- 810/1000 ------ val_loss : [0.78]\n",
      "----------- 820/1000 ------ val_loss : [0.81]\n",
      "----------- 830/1000 ------ val_loss : [0.81]\n",
      "----------- 840/1000 ------ val_loss : [0.83]\n",
      "----------- 850/1000 ------ val_loss : [0.86]\n",
      "----------- 860/1000 ------ val_loss : [0.85]\n",
      "----------- 870/1000 ------ val_loss : [0.85]\n",
      "----------- 880/1000 ------ val_loss : [0.8]\n",
      "----------- 890/1000 ------ val_loss : [0.89]\n",
      "----------- 900/1000 ------ val_loss : [0.9]\n",
      "----------- 910/1000 ------ val_loss : [0.89]\n",
      "----------- 920/1000 ------ val_loss : [0.87]\n",
      "----------- 930/1000 ------ val_loss : [0.92]\n",
      "----------- 940/1000 ------ val_loss : [0.8]\n",
      "----------- 950/1000 ------ val_loss : [0.91]\n",
      "----------- 960/1000 ------ val_loss : [0.94]\n",
      "----------- 970/1000 ------ val_loss : [0.9]\n",
      "----------- 980/1000 ------ val_loss : [0.9]\n",
      "----------- 990/1000 ------ val_loss : [0.84]\n",
      "----------- 1000/1000 ------ val_loss : [0.86]\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrain_memory = read_data()\\nhistory = agent.train(train_memory)\\n'"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_memory = read_data()\n",
    "\n",
    "histories = []\n",
    "print(\"Training...\")\n",
    "for ii in range(1000):\n",
    "    history = agent.train(train_memory)\n",
    "    if ii%10 == 9:\n",
    "        histories.append(history)\n",
    "        print(\"----------- \" +str(ii+1)+\"/1000\"+ \" ------ val_loss : \"+str(history.history['accuracy']))\n",
    "print(\"Complete\")\n",
    "\n",
    "\"\"\"\n",
    "train_memory = read_data()\n",
    "history = agent.train(train_memory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"dqn_imitation_learning_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1079..1354 -> 275-tiles track\n"
     ]
    },
    {
     "ename": "GLException",
     "evalue": "b'op\\xe9ration non valide'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGLException\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-49c7756eb196>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglViewport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVP_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVP_H\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_road\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender_road\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpoly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglEnd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender_indicators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck_glend\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mGLException\u001b[0m: b'op\\xe9ration non valide'"
     ]
    }
   ],
   "source": [
    "# test the agent on the environment\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, new_shape)\n",
    "    for time in range(1000):\n",
    "        env.render()\n",
    "        action = agent.act(state) if (time>warm_up) else 3\n",
    "        next_state, reward, done, _ = env.step(continuous_from_discrete(action))\n",
    "        next_state = np.reshape(next_state, new_shape)\n",
    "        #if agent.in_grass(next_state) and time>warm_up:\n",
    "        #    done = True\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"dqn_imitation_learning_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1078..1355 -> 277-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1223..1533 -> 310-tiles track\n",
      "episode: 0/100, score: 953, e: 1.0\n",
      "WARNING:tensorflow:From C:\\Users\\pierr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Track generation: 1086..1371 -> 285-tiles track\n",
      "episode: 1/100, score: 999, e: 0.99\n",
      "Track generation: 1182..1482 -> 300-tiles track\n",
      "episode: 2/100, score: 999, e: 0.99\n",
      "Track generation: 1098..1376 -> 278-tiles track\n",
      "episode: 3/100, score: 999, e: 0.99\n",
      "Track generation: 1212..1519 -> 307-tiles track\n",
      "episode: 4/100, score: 547, e: 0.98\n",
      "Track generation: 1072..1349 -> 277-tiles track\n",
      "episode: 5/100, score: 428, e: 0.98\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "episode: 6/100, score: 388, e: 0.97\n",
      "Track generation: 1204..1509 -> 305-tiles track\n",
      "episode: 7/100, score: 357, e: 0.97\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "episode: 8/100, score: 379, e: 0.96\n",
      "Track generation: 1131..1418 -> 287-tiles track\n",
      "episode: 9/100, score: 375, e: 0.96\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "episode: 10/100, score: 377, e: 0.95\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "episode: 11/100, score: 379, e: 0.95\n",
      "Track generation: 1058..1326 -> 268-tiles track\n",
      "episode: 12/100, score: 999, e: 0.94\n",
      "Track generation: 1050..1317 -> 267-tiles track\n",
      "episode: 13/100, score: 280, e: 0.94\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "episode: 14/100, score: 379, e: 0.93\n",
      "Track generation: 1240..1554 -> 314-tiles track\n",
      "episode: 15/100, score: 376, e: 0.93\n",
      "Track generation: 1061..1334 -> 273-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1040..1304 -> 264-tiles track\n",
      "episode: 16/100, score: 999, e: 0.92\n",
      "Track generation: 1234..1546 -> 312-tiles track\n",
      "episode: 17/100, score: 380, e: 0.92\n",
      "Track generation: 1274..1597 -> 323-tiles track\n",
      "episode: 18/100, score: 380, e: 0.91\n",
      "Track generation: 1137..1425 -> 288-tiles track\n",
      "episode: 19/100, score: 999, e: 0.91\n",
      "Track generation: 1205..1515 -> 310-tiles track\n",
      "episode: 20/100, score: 165, e: 0.9\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "episode: 21/100, score: 379, e: 0.9\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "episode: 22/100, score: 999, e: 0.9\n",
      "Track generation: 1370..1717 -> 347-tiles track\n",
      "episode: 23/100, score: 999, e: 0.89\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "episode: 24/100, score: 378, e: 0.89\n",
      "Track generation: 1150..1448 -> 298-tiles track\n",
      "episode: 25/100, score: 239, e: 0.88\n",
      "Track generation: 988..1244 -> 256-tiles track\n",
      "episode: 26/100, score: 231, e: 0.88\n",
      "Track generation: 1181..1488 -> 307-tiles track\n",
      "episode: 27/100, score: 256, e: 0.87\n",
      "Track generation: 976..1224 -> 248-tiles track\n",
      "episode: 28/100, score: 378, e: 0.87\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "episode: 29/100, score: 999, e: 0.86\n",
      "Track generation: 1114..1397 -> 283-tiles track\n",
      "episode: 30/100, score: 379, e: 0.86\n",
      "Track generation: 1336..1674 -> 338-tiles track\n",
      "episode: 31/100, score: 999, e: 0.86\n",
      "Track generation: 1131..1418 -> 287-tiles track\n",
      "episode: 32/100, score: 375, e: 0.85\n",
      "Track generation: 1178..1483 -> 305-tiles track\n",
      "episode: 33/100, score: 236, e: 0.85\n",
      "Track generation: 1068..1339 -> 271-tiles track\n",
      "episode: 34/100, score: 378, e: 0.84\n",
      "Track generation: 1299..1628 -> 329-tiles track\n",
      "episode: 35/100, score: 999, e: 0.84\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "episode: 36/100, score: 999, e: 0.83\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "episode: 37/100, score: 378, e: 0.83\n",
      "Track generation: 1121..1405 -> 284-tiles track\n",
      "episode: 38/100, score: 999, e: 0.83\n",
      "Track generation: 1080..1361 -> 281-tiles track\n",
      "episode: 39/100, score: 242, e: 0.82\n",
      "Track generation: 1202..1507 -> 305-tiles track\n",
      "episode: 40/100, score: 376, e: 0.82\n",
      "Track generation: 1178..1476 -> 298-tiles track\n",
      "episode: 41/100, score: 374, e: 0.81\n",
      "Track generation: 1113..1395 -> 282-tiles track\n",
      "episode: 42/100, score: 377, e: 0.81\n",
      "Track generation: 1023..1284 -> 261-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "episode: 43/100, score: 999, e: 0.81\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "episode: 44/100, score: 999, e: 0.8\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "episode: 45/100, score: 376, e: 0.8\n",
      "Track generation: 1203..1514 -> 311-tiles track\n",
      "episode: 46/100, score: 239, e: 0.79\n",
      "Track generation: 1210..1517 -> 307-tiles track\n",
      "episode: 47/100, score: 377, e: 0.79\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "episode: 48/100, score: 377, e: 0.79\n",
      "Track generation: 1079..1352 -> 273-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "episode: 49/100, score: 378, e: 0.78\n",
      "Track generation: 1138..1426 -> 288-tiles track\n",
      "episode: 50/100, score: 999, e: 0.78\n",
      "Track generation: 1282..1606 -> 324-tiles track\n",
      "episode: 51/100, score: 999, e: 0.77\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "episode: 52/100, score: 375, e: 0.77\n",
      "Track generation: 1280..1601 -> 321-tiles track\n",
      "episode: 53/100, score: 379, e: 0.77\n",
      "Track generation: 1058..1322 -> 264-tiles track\n",
      "episode: 54/100, score: 226, e: 0.76\n",
      "Track generation: 1051..1318 -> 267-tiles track\n",
      "episode: 55/100, score: 376, e: 0.76\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "episode: 56/100, score: 378, e: 0.76\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "episode: 57/100, score: 378, e: 0.75\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "episode: 58/100, score: 376, e: 0.75\n",
      "Track generation: 1253..1578 -> 325-tiles track\n",
      "episode: 59/100, score: 251, e: 0.74\n",
      "Track generation: 1137..1432 -> 295-tiles track\n",
      "episode: 60/100, score: 243, e: 0.74\n",
      "Track generation: 1286..1611 -> 325-tiles track\n",
      "episode: 61/100, score: 999, e: 0.74\n",
      "Track generation: 1100..1379 -> 279-tiles track\n",
      "episode: 62/100, score: 377, e: 0.73\n",
      "Track generation: 1226..1544 -> 318-tiles track\n",
      "episode: 63/100, score: 176, e: 0.73\n",
      "Track generation: 1038..1307 -> 269-tiles track\n",
      "episode: 64/100, score: 230, e: 0.73\n",
      "Track generation: 1041..1306 -> 265-tiles track\n",
      "episode: 65/100, score: 377, e: 0.72\n",
      "Track generation: 1286..1617 -> 331-tiles track\n",
      "episode: 66/100, score: 167, e: 0.72\n",
      "Track generation: 1173..1475 -> 302-tiles track\n",
      "episode: 67/100, score: 164, e: 0.71\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "episode: 68/100, score: 378, e: 0.71\n",
      "Track generation: 1052..1320 -> 268-tiles track\n",
      "episode: 69/100, score: 999, e: 0.71\n",
      "Track generation: 1255..1573 -> 318-tiles track\n",
      "episode: 70/100, score: 376, e: 0.7\n",
      "Track generation: 1335..1673 -> 338-tiles track\n",
      "episode: 71/100, score: 999, e: 0.7\n",
      "Track generation: 1082..1356 -> 274-tiles track\n",
      "episode: 72/100, score: 377, e: 0.7\n",
      "Track generation: 1102..1390 -> 288-tiles track\n",
      "episode: 73/100, score: 255, e: 0.69\n",
      "Track generation: 1127..1415 -> 288-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "episode: 74/100, score: 377, e: 0.69\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "episode: 75/100, score: 375, e: 0.69\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "episode: 76/100, score: 376, e: 0.68\n",
      "Track generation: 1184..1484 -> 300-tiles track\n",
      "episode: 77/100, score: 374, e: 0.68\n",
      "Track generation: 1098..1383 -> 285-tiles track\n",
      "episode: 78/100, score: 239, e: 0.68\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "episode: 79/100, score: 374, e: 0.67\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "episode: 80/100, score: 375, e: 0.67\n",
      "Track generation: 1132..1429 -> 297-tiles track\n",
      "episode: 81/100, score: 278, e: 0.67\n",
      "Track generation: 935..1172 -> 237-tiles track\n",
      "episode: 82/100, score: 999, e: 0.66\n",
      "Track generation: 1222..1532 -> 310-tiles track\n",
      "episode: 83/100, score: 376, e: 0.66\n",
      "Track generation: 1152..1443 -> 291-tiles track\n",
      "episode: 84/100, score: 375, e: 0.66\n",
      "Track generation: 1082..1357 -> 275-tiles track\n",
      "episode: 85/100, score: 999, e: 0.65\n",
      "Track generation: 1184..1484 -> 300-tiles track\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 86/100, score: 375, e: 0.65\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "episode: 87/100, score: 999, e: 0.65\n",
      "Track generation: 1115..1398 -> 283-tiles track\n",
      "episode: 88/100, score: 376, e: 0.64\n",
      "Track generation: 1099..1378 -> 279-tiles track\n",
      "episode: 89/100, score: 375, e: 0.64\n",
      "Track generation: 1040..1311 -> 271-tiles track\n",
      "episode: 90/100, score: 173, e: 0.64\n",
      "Track generation: 1134..1428 -> 294-tiles track\n",
      "episode: 91/100, score: 239, e: 0.63\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "episode: 92/100, score: 376, e: 0.63\n",
      "Track generation: 1116..1406 -> 290-tiles track\n",
      "episode: 93/100, score: 178, e: 0.63\n",
      "Track generation: 1223..1533 -> 310-tiles track\n",
      "episode: 94/100, score: 999, e: 0.62\n",
      "Track generation: 1253..1570 -> 317-tiles track\n",
      "episode: 95/100, score: 999, e: 0.62\n",
      "Track generation: 1076..1355 -> 279-tiles track\n",
      "episode: 96/100, score: 231, e: 0.62\n",
      "Track generation: 1255..1573 -> 318-tiles track\n",
      "episode: 97/100, score: 380, e: 0.61\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "episode: 98/100, score: 379, e: 0.61\n",
      "Track generation: 1085..1362 -> 277-tiles track\n",
      "episode: 99/100, score: 380, e: 0.61\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 100\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, new_shape)\n",
    "    for time in range(1000):\n",
    "        action = agent.act(state) if (time>warm_up) else 3\n",
    "        next_state, reward, done, _ = env.step(continuous_from_discrete(action))\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, new_shape)\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "            \n",
    "agent.save(\"dqn_imitation_learning_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 96)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.imshow(test, cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80701834, 0.01145028, 0.3124179 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8295293 ,  0.49595413,  0.3533085 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}