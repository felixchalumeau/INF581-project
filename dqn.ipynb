{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the environment\n",
    "import gym\n",
    "env = gym.make('CarRacing-v0')\n",
    "\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, sgd\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import pickle, os, gzip\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape : (96, 96, 3)\n",
      "New shape : (1, 96, 96, 3)\n",
      "New prrocessed shape : (1, 84, 84, 1)\n",
      "Continuous action shape : 3\n",
      "Action shape : 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "processed_shape = (84,84,1)\n",
    "new_shape = (1,)+state_size\n",
    "new_processed_shape= (1,)+processed_shape\n",
    "continuous_action_size = env.action_space.shape[0]\n",
    "action_size = 4\n",
    "\n",
    "print(\"State shape : \" + str(state_size))\n",
    "print(\"New shape : \" + str(new_shape))\n",
    "print(\"New preprocessed shape : \" + str(new_processed_shape))\n",
    "print(\"Continuous action shape : \" + str(continuous_action_size))\n",
    "print(\"Action shape : \" + str(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(observation)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(observation[:84,6:90,1] , cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_from_discrete(action):\n",
    "    if (action==0):\n",
    "        return [-1, 0, 0]\n",
    "    if (action==1):\n",
    "        return [1, 0, 0]\n",
    "    if (action==2):\n",
    "        return [0, 0, 0]\n",
    "    #other actions that might be taken into account [-1, 0, 0.5]  [1, 0, 0.5]  [0, 1, 0]  [0, 0, 0.5]\n",
    "    return [0, 1, 0]\n",
    "\n",
    "def discrete_from_continuous(action):\n",
    "    if (action[0] == -1):\n",
    "        return 0\n",
    "    if (action[0] == 1):\n",
    "        return 1\n",
    "    if (action[1] == 0):\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "def onehot_from_discrete(action):\n",
    "    return [ 1 if i==action else 0 for i in range(action_size)]\n",
    "\n",
    "def discrete_from_onehot(onehot):\n",
    "    for i in range(len(onehot)):\n",
    "        if (onehot[i]==1):\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    return np.reshape(state[:,:84,6:90,1], new_processed_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 0.3  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "        # this paper gave a nn architecture\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters = 32, kernel_size=8, strides=4, activation='relu', input_shape=processed_shape))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Conv2D(filters = 64, kernel_size=4, strides=2, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',metrics=['accuracy'], optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon = -1):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = preprocess(state)\n",
    "        act_values = self.model.predict(state)[0]\n",
    "        return np.argmax(act_values)  # returns action\n",
    "    \n",
    "    def in_grass(self, state):\n",
    "        return state[0,82,42,1]>150\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size=100\n",
    "        states = []\n",
    "        target_fs = []\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            next_state = preprocess(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state)\n",
    "            target_fs.append(target_f)\n",
    "        history = self.model.fit(np.array(states)[:,0,:,:,:],\n",
    "                                 np.array(target_fs)[:,0,:],\n",
    "                                 batch_size=batch_size,\n",
    "                                 verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return history\n",
    "            \n",
    "    def train(self, memory):\n",
    "        batch_size=100\n",
    "        states = []\n",
    "        target_fs = []\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            next_state = preprocess(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state)\n",
    "            target_fs.append(target_f)\n",
    "        history = self.model.fit(np.array(states)[:,0,:,:,:],\n",
    "                                 np.array(target_fs)[:,0,:],\n",
    "                                 batch_size=batch_size,\n",
    "                                 verbose=0)\n",
    "        return history\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters = 32, kernel_size=(8,8), strides=(2,2), activation='relu', input_shape=processed_shape))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Conv2D(filters = 64, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = preprocess(state)\n",
    "        act_values = self.model.predict(state)[0]\n",
    "        return np.argmax(act_values)  # returns action\n",
    "    \n",
    "    def in_grass(self, state):\n",
    "        return state[0,82,42,1]>150\n",
    "\n",
    "    def train(self, memory):\n",
    "        batch_size=100\n",
    "        states = []\n",
    "        actions = []\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            action = onehot_from_discrete(action)\n",
    "            states.append(state)\n",
    "            actions.append([action])\n",
    "        states = np.array(states)[:,0,:,:,:]\n",
    "        actions = np.array(actions)[:,0,:]\n",
    "        history = self.model.fit(states, actions, batch_size=100, verbose = 0)\n",
    "        return history\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 0.3  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.tau = 0.5\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters = 32, kernel_size=8, strides=4, activation='relu', input_shape=processed_shape))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Conv2D(filters = 64, kernel_size=4, strides=2, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',metrics=['accuracy'], optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon = -1):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = preprocess(state)\n",
    "        act_values = self.model.predict(state)[0]\n",
    "        return np.argmax(act_values)  # returns action\n",
    "    \n",
    "    def in_grass(self, state):\n",
    "        return state[0,82,42,1]>150\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size=100\n",
    "        states = []\n",
    "        target_fs = []\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            next_state = preprocess(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target_next = self.model.predict(next_state)\n",
    "                target_val = self.target_model.predict(next_state)\n",
    "                a = np.argmax(target_next[0])\n",
    "                target = reward + self.gamma * (target_val[0][a])  \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state)\n",
    "            target_fs.append(target_f)\n",
    "        history = self.model.fit(np.array(states)[:,0,:,:,:],\n",
    "                                 np.array(target_fs)[:,0,:],\n",
    "                                 batch_size=batch_size,\n",
    "                                 verbose=0)\n",
    "        q_model_theta = self.model.get_weights()\n",
    "        target_model_theta = self.target_model.get_weights()\n",
    "        counter = 0\n",
    "        for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
    "            target_weight = target_weight * (1-self.tau) + q_weight * self.tau\n",
    "            target_model_theta[counter] = target_weight\n",
    "            counter += 1\n",
    "        self.target_model.set_weights(target_model_theta)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return history\n",
    "            \n",
    "    def train(self, memory):\n",
    "        batch_size=100\n",
    "        states = []\n",
    "        target_fs = []\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = preprocess(state)\n",
    "            next_state = preprocess(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target_next = self.model.predict(next_state)\n",
    "                target_val = self.target_model.predict(next_state)\n",
    "                a = np.argmax(target_next[0])\n",
    "                target = reward + self.gamma * (target_val[0][a])  \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state)\n",
    "            target_fs.append(target_f)\n",
    "        history = self.model.fit(np.array(states)[:,0,:,:,:],\n",
    "                                 np.array(target_fs)[:,0,:],\n",
    "                                 batch_size=batch_size,\n",
    "                                 verbose=0)\n",
    "        '''\n",
    "        # Soft Update\n",
    "        q_model_theta = self.model.get_weights()\n",
    "        target_model_theta = self.target_model.get_weights()\n",
    "        counter = 0\n",
    "        for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
    "            target_weight = target_weight * (1-self.tau) + q_weight * self.tau\n",
    "            target_model_theta[counter] = target_weight\n",
    "            counter += 1\n",
    "        self.target_model.set_weights(target_model_theta)\n",
    "        '''\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        return history\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.target_model.load_weights(\"target_\"+name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        self.target_model.save_weights(\"target_\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    #get the data needed from the user gzip\n",
    "    print(\"Reading data...\")\n",
    "    with gzip.open('./data/data.pkl.gzip','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    states = data[\"state\"]\n",
    "    next_states = data[\"next_state\"]\n",
    "    actions = data[\"action\"]\n",
    "    rewards = data[\"reward\"]\n",
    "    dones = data[\"terminal\"]\n",
    "    \n",
    "    # put it in the good format, as in the agent memory\n",
    "    memory = deque(maxlen=100000)\n",
    "    NB_TRIALS = len(states)\n",
    "    for ii in range(NB_TRIALS):\n",
    "        for jj in range(warm_up,len(states[ii])):\n",
    "            state = states[ii][jj]\n",
    "            state = np.reshape(state, new_shape)\n",
    "            action = discrete_from_continuous(actions[ii][jj])\n",
    "            reward = rewards[ii][jj]\n",
    "            next_state = next_states[ii][jj]\n",
    "            next_state = np.reshape(next_state, new_shape)\n",
    "            done = dones[ii][jj]\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 20, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               663680    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 699,108\n",
      "Trainable params: 699,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size, action_size)\n",
    "# agent.load(\"dqn_imitation_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Training...\n",
      "Track generation: 1221..1530 -> 309-tiles track\n",
      "Track generation: 1097..1375 -> 278-tiles track\n",
      "Track generation: 1169..1465 -> 296-tiles track\n",
      "Track generation: 1044..1309 -> 265-tiles track\n",
      "Track generation: 973..1227 -> 254-tiles track\n",
      "Track generation: 1167..1466 -> 299-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1103..1392 -> 289-tiles track\n",
      "Track generation: 1190..1492 -> 302-tiles track\n",
      "Track generation: 1223..1533 -> 310-tiles track\n",
      "Track generation: 1189..1490 -> 301-tiles track\n",
      "Track generation: 1146..1439 -> 293-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1164..1454 -> 290-tiles track\n",
      "1:0.59\n",
      "1:-45.25260655149099\n",
      "Track generation: 1018..1277 -> 259-tiles track\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1327..1663 -> 336-tiles track\n",
      "Track generation: 1095..1373 -> 278-tiles track\n",
      "Track generation: 1216..1524 -> 308-tiles track\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Track generation: 1105..1393 -> 288-tiles track\n",
      "Track generation: 963..1211 -> 248-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1219..1528 -> 309-tiles track\n",
      "Track generation: 1047..1313 -> 266-tiles track\n",
      "Track generation: 1213..1520 -> 307-tiles track\n",
      "51:0.94\n",
      "51:90.2024559825946\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1179..1478 -> 299-tiles track\n",
      "Track generation: 1212..1519 -> 307-tiles track\n",
      "Track generation: 1176..1477 -> 301-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1107..1388 -> 281-tiles track\n",
      "Track generation: 1238..1552 -> 314-tiles track\n",
      "Track generation: 1215..1523 -> 308-tiles track\n",
      "Track generation: 1110..1391 -> 281-tiles track\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Track generation: 1151..1444 -> 293-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1239..1552 -> 313-tiles track\n",
      "Track generation: 1110..1392 -> 282-tiles track\n",
      "101:0.88\n",
      "101:74.60372093274465\n",
      "Track generation: 1005..1266 -> 261-tiles track\n",
      "Track generation: 1121..1404 -> 283-tiles track\n",
      "Track generation: 1101..1380 -> 279-tiles track\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1059..1328 -> 269-tiles track\n",
      "Track generation: 1245..1565 -> 320-tiles track\n",
      "Track generation: 1325..1660 -> 335-tiles track\n",
      "Track generation: 1145..1436 -> 291-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "151:0.85\n",
      "151:81.78894934925282\n",
      "Track generation: 1051..1318 -> 267-tiles track\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1092..1376 -> 284-tiles track\n",
      "Track generation: 1166..1461 -> 295-tiles track\n",
      "Track generation: 1208..1524 -> 316-tiles track\n",
      "Track generation: 1196..1499 -> 303-tiles track\n",
      "Track generation: 1059..1328 -> 269-tiles track\n",
      "Track generation: 1275..1600 -> 325-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1174..1471 -> 297-tiles track\n",
      "Track generation: 1187..1498 -> 311-tiles track\n",
      "201:0.79\n",
      "201:22.910095822463923\n",
      "Track generation: 1257..1576 -> 319-tiles track\n",
      "Track generation: 1073..1349 -> 276-tiles track\n",
      "Track generation: 1221..1538 -> 317-tiles track\n",
      "Track generation: 1026..1295 -> 269-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Track generation: 1127..1413 -> 286-tiles track\n",
      "Track generation: 1032..1301 -> 269-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 901..1135 -> 234-tiles track\n",
      "251:0.9\n",
      "251:6.948739691057739\n",
      "Track generation: 1228..1543 -> 315-tiles track\n",
      "Track generation: 1268..1589 -> 321-tiles track\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "Track generation: 1260..1579 -> 319-tiles track\n",
      "Track generation: 1107..1390 -> 283-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 973..1225 -> 252-tiles track\n",
      "Track generation: 1420..1779 -> 359-tiles track\n",
      "Track generation: 1120..1408 -> 288-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1032..1303 -> 271-tiles track\n",
      "301:0.94\n",
      "301:-42.855783757626746\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1065..1330 -> 265-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1073..1345 -> 272-tiles track\n",
      "Track generation: 1116..1407 -> 291-tiles track\n",
      "Track generation: 1070..1343 -> 273-tiles track\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 1124..1397 -> 273-tiles track\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Track generation: 1239..1553 -> 314-tiles track\n",
      "Track generation: 1106..1392 -> 286-tiles track\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "351:0.86\n",
      "351:54.5403462350158\n",
      "Track generation: 1050..1317 -> 267-tiles track\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Track generation: 1158..1452 -> 294-tiles track\n",
      "Track generation: 1039..1312 -> 273-tiles track\n",
      "Track generation: 1424..1784 -> 360-tiles track\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Track generation: 1262..1581 -> 319-tiles track\n",
      "Track generation: 1025..1290 -> 265-tiles track\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "401:0.81\n",
      "401:36.282520362154855\n",
      "Track generation: 1109..1390 -> 281-tiles track\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Track generation: 1256..1574 -> 318-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1387..1738 -> 351-tiles track\n",
      "Track generation: 1200..1504 -> 304-tiles track\n",
      "Track generation: 1303..1633 -> 330-tiles track\n",
      "Track generation: 913..1152 -> 239-tiles track\n",
      "Track generation: 1101..1380 -> 279-tiles track\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "451:0.79\n",
      "451:42.076649702728645\n",
      "Track generation: 989..1245 -> 256-tiles track\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "Track generation: 1049..1316 -> 267-tiles track\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "Track generation: 1221..1530 -> 309-tiles track\n",
      "Track generation: 1376..1718 -> 342-tiles track\n",
      "Track generation: 1099..1386 -> 287-tiles track\n",
      "Track generation: 1216..1524 -> 308-tiles track\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 1244..1559 -> 315-tiles track\n",
      "501:0.73\n",
      "501:16.21328360359889\n",
      "Track generation: 1065..1336 -> 271-tiles track\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1216..1524 -> 308-tiles track\n",
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Track generation: 1118..1401 -> 283-tiles track\n",
      "Track generation: 1105..1385 -> 280-tiles track\n",
      "Track generation: 1137..1425 -> 288-tiles track\n",
      "Track generation: 1113..1395 -> 282-tiles track\n",
      "Track generation: 1237..1551 -> 314-tiles track\n",
      "Track generation: 1246..1561 -> 315-tiles track\n",
      "551:0.79\n",
      "551:71.36670487356528\n",
      "Track generation: 1358..1701 -> 343-tiles track\n",
      "Track generation: 988..1239 -> 251-tiles track\n",
      "Track generation: 1240..1554 -> 314-tiles track\n",
      "Track generation: 1079..1357 -> 278-tiles track\n",
      "Track generation: 1125..1413 -> 288-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1281..1610 -> 329-tiles track\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "Track generation: 1052..1319 -> 267-tiles track\n",
      "Track generation: 1116..1399 -> 283-tiles track\n",
      "Track generation: 1363..1710 -> 347-tiles track\n",
      "Track generation: 1099..1378 -> 279-tiles track\n",
      "601:0.71\n",
      "601:41.50179390141644\n",
      "Track generation: 1138..1435 -> 297-tiles track\n",
      "Track generation: 1166..1433 -> 267-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1061..1337 -> 276-tiles track\n",
      "Track generation: 1169..1467 -> 298-tiles track\n",
      "Track generation: 1131..1424 -> 293-tiles track\n",
      "Track generation: 1176..1477 -> 301-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1017..1275 -> 258-tiles track\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 949..1196 -> 247-tiles track\n",
      "Track generation: 1107..1388 -> 281-tiles track\n",
      "Track generation: 1219..1528 -> 309-tiles track\n",
      "Track generation: 1121..1405 -> 284-tiles track\n",
      "Track generation: 1313..1645 -> 332-tiles track\n",
      "651:0.75\n",
      "651:24.436219035875475\n",
      "Track generation: 1341..1680 -> 339-tiles track\n",
      "Track generation: 1118..1402 -> 284-tiles track\n",
      "Track generation: 1005..1266 -> 261-tiles track\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Track generation: 1248..1564 -> 316-tiles track\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1043..1308 -> 265-tiles track\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "Track generation: 1177..1476 -> 299-tiles track\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "701:0.71\n",
      "701:67.47341224478603\n",
      "Track generation: 1220..1529 -> 309-tiles track\n",
      "Track generation: 1155..1456 -> 301-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 980..1229 -> 249-tiles track\n",
      "Track generation: 969..1220 -> 251-tiles track\n",
      "Track generation: 1299..1628 -> 329-tiles track\n",
      "Track generation: 1369..1715 -> 346-tiles track\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Track generation: 1042..1311 -> 269-tiles track\n",
      "Track generation: 1254..1573 -> 319-tiles track\n",
      "751:0.74\n",
      "751:80.64168850174244\n",
      "Track generation: 1084..1358 -> 274-tiles track\n",
      "Track generation: 934..1178 -> 244-tiles track\n",
      "Track generation: 1124..1409 -> 285-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Track generation: 1246..1562 -> 316-tiles track\n",
      "Track generation: 1187..1487 -> 300-tiles track\n",
      "Track generation: 1174..1472 -> 298-tiles track\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1336..1675 -> 339-tiles track\n",
      "801:0.73\n",
      "801:15.735996044372802\n",
      "Track generation: 1185..1485 -> 300-tiles track\n",
      "Track generation: 1443..1808 -> 365-tiles track\n",
      "Track generation: 1325..1660 -> 335-tiles track\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-3307c9fe522c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mwarm_up\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontinuous_from_discrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglViewport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVP_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVP_H\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_road\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender_road\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpoly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroad_poly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglColor4f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_memory = read_data()\n",
    "\n",
    "print(\"Training...\")\n",
    "for ii in range(1000):\n",
    "    history = agent.train(train_memory)\n",
    "    if ii%50 == 0:  # Compute the score on an average of 10 games\n",
    "        scores = []\n",
    "        for e in range(10):\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, new_shape)\n",
    "            for time in range(1000):\n",
    "                action = agent.act(state) if (time>warm_up) else 3\n",
    "                next_state, reward, done, _ = env.step(continuous_from_discrete(action))\n",
    "                next_state = np.reshape(next_state, new_shape)\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            scores.append(score)\n",
    "        avg = np.mean(scores)\n",
    "        print(str(ii+1) + \" - nn accuracy:\" + str(history.history['accuracy'][0]))\n",
    "        print(str(ii+1) + \" - agent score:\" + str(avg))\n",
    "print(\"Complete\")\n",
    "\n",
    "\"\"\"\n",
    "train_memory = read_data()\n",
    "history = agent.train(train_memory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"dqn_imitation_learning_v5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1200..1505 -> 305-tiles track\n",
      "Track generation: 1065..1338 -> 273-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1004..1259 -> 255-tiles track\n",
      "Track generation: 1236..1548 -> 312-tiles track\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "Track generation: 1162..1457 -> 295-tiles track\n",
      "Track generation: 1009..1265 -> 256-tiles track\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Track generation: 1324..1659 -> 335-tiles track\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Track generation: 1121..1414 -> 293-tiles track\n",
      "Track generation: 1249..1565 -> 316-tiles track\n",
      "Track generation: 1059..1328 -> 269-tiles track\n",
      "Track generation: 1148..1439 -> 291-tiles track\n",
      "Track generation: 1091..1369 -> 278-tiles track\n",
      "Track generation: 1355..1698 -> 343-tiles track\n",
      "Track generation: 1245..1551 -> 306-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1287..1613 -> 326-tiles track\n",
      "Track generation: 1065..1335 -> 270-tiles track\n",
      "Track generation: 1231..1550 -> 319-tiles track\n",
      "Track generation: 1311..1642 -> 331-tiles track\n",
      "Track generation: 995..1249 -> 254-tiles track\n",
      "Track generation: 1139..1435 -> 296-tiles track\n",
      "Track generation: 1080..1354 -> 274-tiles track\n",
      "Track generation: 986..1244 -> 258-tiles track\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1139..1430 -> 291-tiles track\n",
      "Track generation: 1244..1560 -> 316-tiles track\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Track generation: 1097..1380 -> 283-tiles track\n",
      "Track generation: 1162..1457 -> 295-tiles track\n",
      "Track generation: 1004..1259 -> 255-tiles track\n",
      "Track generation: 1176..1475 -> 299-tiles track\n",
      "Track generation: 1150..1450 -> 300-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1137..1425 -> 288-tiles track\n",
      "Track generation: 1092..1369 -> 277-tiles track\n",
      "Track generation: 1365..1710 -> 345-tiles track\n",
      "Track generation: 1199..1506 -> 307-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Track generation: 999..1255 -> 256-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1173..1478 -> 305-tiles track\n",
      "Track generation: 1125..1410 -> 285-tiles track\n",
      "Track generation: 1266..1587 -> 321-tiles track\n",
      "Track generation: 1260..1578 -> 318-tiles track\n",
      "Track generation: 1083..1358 -> 275-tiles track\n",
      "Track generation: 1116..1402 -> 286-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1146..1444 -> 298-tiles track\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "Track generation: 979..1232 -> 253-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1293..1621 -> 328-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1156..1456 -> 300-tiles track\n",
      "Track generation: 1232..1545 -> 313-tiles track\n",
      "Track generation: 948..1195 -> 247-tiles track\n",
      "Track generation: 1083..1358 -> 275-tiles track\n",
      "Track generation: 1369..1711 -> 342-tiles track\n",
      "Track generation: 1137..1425 -> 288-tiles track\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Track generation: 1223..1533 -> 310-tiles track\n",
      "Track generation: 1048..1320 -> 272-tiles track\n",
      "Track generation: 1219..1528 -> 309-tiles track\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "Track generation: 1024..1284 -> 260-tiles track\n",
      "Track generation: 999..1255 -> 256-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1059..1326 -> 267-tiles track\n",
      "Track generation: 1153..1445 -> 292-tiles track\n",
      "Track generation: 1086..1362 -> 276-tiles track\n",
      "Track generation: 986..1244 -> 258-tiles track\n",
      "Track generation: 1086..1373 -> 287-tiles track\n",
      "Track generation: 1024..1286 -> 262-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Track generation: 1159..1459 -> 300-tiles track\n",
      "Track generation: 1111..1393 -> 282-tiles track\n",
      "Track generation: 1030..1298 -> 268-tiles track\n",
      "Track generation: 1178..1477 -> 299-tiles track\n",
      "Track generation: 1115..1398 -> 283-tiles track\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1113..1396 -> 283-tiles track\n",
      "Track generation: 1017..1279 -> 262-tiles track\n",
      "Track generation: 1191..1493 -> 302-tiles track\n",
      "Track generation: 922..1157 -> 235-tiles track\n",
      "Track generation: 1316..1649 -> 333-tiles track\n",
      "Track generation: 1113..1395 -> 282-tiles track\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Track generation: 1391..1743 -> 352-tiles track\n",
      "Track generation: 1158..1452 -> 294-tiles track\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Track generation: 1499..1878 -> 379-tiles track\n",
      "Track generation: 1175..1474 -> 299-tiles track\n",
      "Track generation: 1089..1368 -> 279-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1121..1411 -> 290-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "Track generation: 1276..1599 -> 323-tiles track\n",
      "Track generation: 1110..1394 -> 284-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1244..1559 -> 315-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1071..1343 -> 272-tiles track\n",
      "Track generation: 1196..1499 -> 303-tiles track\n",
      "Track generation: 1059..1336 -> 277-tiles track\n",
      "Track generation: 1080..1354 -> 274-tiles track\n",
      "Track generation: 1247..1563 -> 316-tiles track\n",
      "Track generation: 1103..1383 -> 280-tiles track\n",
      "Track generation: 1240..1554 -> 314-tiles track\n",
      "Track generation: 1176..1477 -> 301-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "301:814.0092598967041\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for e in range(100):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, new_shape)\n",
    "    for time in range(1000):\n",
    "        action = agent.act(state) if (time>warm_up) else 3\n",
    "        next_state, reward, done, _ = env.step(continuous_from_discrete(action))\n",
    "        next_state = np.reshape(next_state, new_shape)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    scores.append(score)\n",
    "avg = np.mean(scores)\n",
    "print(str(ii+1) + \":\" + str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1039..1303 -> 264-tiles track\n",
      "episode: 0/1000, score: 884.7908745246934, e: 0.3\n",
      "Track generation: 1247..1563 -> 316-tiles track\n",
      "episode: 1/1000, score: 811.1111111110937, e: 0.3\n"
     ]
    }
   ],
   "source": [
    "# test the agent on the environment\n",
    "\n",
    "for e in range(2):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, new_shape)\n",
    "    for time in range(1000):\n",
    "        env.render()\n",
    "        action = agent.act(state) if (time>32) else 3\n",
    "        next_state, reward, done, _ = env.step(continuous_from_discrete(action))\n",
    "        next_state = np.reshape(next_state, new_shape)\n",
    "        score += reward\n",
    "        #if agent.in_grass(next_state) and time>warm_up:\n",
    "        #    done = True\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, score, agent.epsilon))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the agent after the learning from demonstration part (not efficient - the agent unlearn how to turn)\n",
    "EPISODES = 100\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, new_shape)\n",
    "    for time in range(1000):\n",
    "        env.render()\n",
    "        action = agent.act(state, agent.epsilon) if (time>warm_up) else 3\n",
    "        next_state, reward, done, _ = env.step(continuous_from_discrete(action))\n",
    "        next_state = np.reshape(next_state, new_shape)\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    history = agent.replay()\n",
    "    print(\"----------- \" +str(e)+\"/\"+str(EPISODES)+ \" ------ val_loss : \"+str(history.history['accuracy']))\n",
    "            \n",
    "agent.save(\"dqn_imitation_learning_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
