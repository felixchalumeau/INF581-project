{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful informations from an observation\n",
    "def capteur(observation):\n",
    "    '''\n",
    "    Position du nez de la voiture codé en dur pour l'instant.\n",
    "    Cette fonction renvoie les quatres distances d'intérêt !\n",
    "    '''\n",
    "    i_nose = 67\n",
    "    j_left = 46\n",
    "    j_right = 49\n",
    "    \n",
    "    grass_color = 180\n",
    "    road_color = 102\n",
    "\n",
    "    # informations horizontales\n",
    "    horizontal = np.where(observation[67] == grass_color)[0]\n",
    "    hor_road = np.where(observation[67] == road_color)[0]\n",
    "    try:\n",
    "        hori_gauche = 46 - horizontal[horizontal < 46][-1]\n",
    "        if (hori_gauche == 1):\n",
    "            hori_gauche = 49 - hor_road[hor_road > 49][0]\n",
    "    except:\n",
    "        hori_gauche = 46\n",
    "    try:\n",
    "        hori_droite = horizontal[horizontal > 49][0] - 49\n",
    "        if (hori_droite == 1):\n",
    "            hori_droite = hor_road[hor_road < 46][-1] - 46\n",
    "    except:\n",
    "        hori_droite = 46\n",
    "    \n",
    "    # informations verticales\n",
    "    vertical_gauche = np.where(observation[:, 46] == grass_color)[0]\n",
    "    try:\n",
    "        verti_gauche = 67 - vertical_gauche[vertical_gauche < 67][-1] \n",
    "    except:\n",
    "        verti_gauche = 67\n",
    "    verti_gauche = (verti_gauche -33) / 33\n",
    "\n",
    "    vertical_droite = np.where(observation[:, 49] == grass_color)[0]\n",
    "    try:\n",
    "        verti_droite = 67 - vertical_droite[vertical_droite < 67][-1]\n",
    "    except:\n",
    "        verti_droite = 67\n",
    "        \n",
    "    verti_droite = (verti_droite - 33) / 33\n",
    "\n",
    "    res = np.array([hori_gauche, hori_droite, verti_gauche, verti_droite])\n",
    "\n",
    "    return res/20\n",
    "\n",
    "def preprocess(rgb):\n",
    "    '''\n",
    "    Simplifie l'image. PAsse de RGB à gray et enlève le base inutile.\n",
    "    '''\n",
    "    end_img = 84\n",
    "    \n",
    "    gray = np.dot(rgb[...,:3], [0.0, 0.5, 0.5])\n",
    "    gray[gray>150] = 180\n",
    "    return capteur(gray[:84])\n",
    "\n",
    "def processAction(output):\n",
    "    action = [0, 0, 0]\n",
    "    action[0] = (output[0] - 0.5)*2\n",
    "    if (output[1] >= 0.5):\n",
    "        action[1] = (output[1] - 0.5)*2\n",
    "    else:\n",
    "        action[2] = (0.5 - output[1])*2\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRacingAI(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.lin = nn.Sequential(\n",
    "                nn.Linear(4,128, bias=True),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,game_actions, bias=True),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "                \n",
    "        def forward(self, inputs):\n",
    "            x = self.lin(inputs)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):        \n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = CarRacingAI()\n",
    "        agent = agent.float()\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "    \n",
    "        observation = env.reset()\n",
    "        \n",
    "        r=0\n",
    "        \n",
    "        for _ in range(500):\n",
    "            observation = np.ascontiguousarray(observation)\n",
    "            observation = preprocess(observation)\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor')\n",
    "            output = agent(inp).detach().numpy()\n",
    "            action = processAction(output)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r=r+reward\n",
    "            \n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)      \n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        avg_score.append(return_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1069..1342 -> 273-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1240..1554 -> 314-tiles track\n",
      "Track generation: 1197..1501 -> 304-tiles track\n",
      "Track generation: 1204..1512 -> 308-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1232..1548 -> 316-tiles track\n",
      "Track generation: 1132..1427 -> 295-tiles track\n",
      "Track generation: 1105..1395 -> 290-tiles track\n",
      "Track generation: 1185..1485 -> 300-tiles track\n",
      "Track generation: 1048..1314 -> 266-tiles track\n",
      "Track generation: 1124..1409 -> 285-tiles track\n"
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 30\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 5\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 100\n",
    "\n",
    "elite_index = None\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards = run_agents_n_times(agents, 3) #return average of 3 runs\n",
    "\n",
    "    # sort by rewards\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit] #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    top_rewards = []\n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean reward: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "    #print(rewards)\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    \n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "    try: #try and exception block because, render hangs if an erorr occurs, we must do env.close to continue working    \n",
    "        \n",
    "        observation = env.reset()\n",
    "        \n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        for _ in range(1000):\n",
    "            env.render()\n",
    "            observation = np.ascontiguousarray(observation)\n",
    "            observation = preprocess(observation)\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor')\n",
    "            output = agent(inp).detach().numpy()\n",
    "            action = processAction(output)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r=r+reward\n",
    "            \n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "        print(\"Rewards: \",r)\n",
    "\n",
    "    except Exception as e:\n",
    "        env.close()\n",
    "        print(e.__doc__)\n",
    "        print(e.message)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_agent(agents[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
